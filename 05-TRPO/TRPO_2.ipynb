{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyO2xIdK4hgomTqNE2UKATUM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HarryHy/RL-Practice/blob/main/TRPO/TRPO_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8rpArRBhFndX"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "oApyxYQLFuzW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import torch\n",
        "import collections\n",
        "import random\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        transitions = random.sample(self.buffer, batch_size)\n",
        "        state, action, reward, next_state, done = zip(*transitions)\n",
        "        return np.array(state), action, reward, np.array(next_state), done\n",
        "\n",
        "    def size(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "def moving_average(a, window_size):\n",
        "    cumulative_sum = np.cumsum(np.insert(a, 0, 0))\n",
        "    middle = (cumulative_sum[window_size:] - cumulative_sum[:-window_size]) / window_size\n",
        "    r = np.arange(1, window_size-1, 2)\n",
        "    begin = np.cumsum(a[:window_size-1])[::2] / r\n",
        "    end = (np.cumsum(a[:-window_size:-1])[::2] / r)[::-1]\n",
        "    return np.concatenate((begin, middle, end))\n",
        "\n",
        "def train_on_policy_agent(env, agent, num_episodes):\n",
        "    return_list = []\n",
        "    for i in range(10):\n",
        "        with tqdm(total=int(num_episodes/10), desc='Iteration %d' % i) as pbar:\n",
        "            for i_episode in range(int(num_episodes/10)):\n",
        "                episode_return = 0\n",
        "                transition_dict = {'states': [], 'actions': [], 'next_states': [], 'rewards': [], 'dones': []}\n",
        "                state = env.reset()\n",
        "                done = False\n",
        "                while not done:\n",
        "                    action = agent.take_action(state)\n",
        "                    next_state, reward, done, _ = env.step(action)\n",
        "                    transition_dict['states'].append(state)\n",
        "                    transition_dict['actions'].append(action)\n",
        "                    transition_dict['next_states'].append(next_state)\n",
        "                    transition_dict['rewards'].append(reward)\n",
        "                    transition_dict['dones'].append(done)\n",
        "                    state = next_state\n",
        "                    episode_return += reward\n",
        "                return_list.append(episode_return)\n",
        "                agent.update(transition_dict)\n",
        "                if (i_episode+1) % 10 == 0:\n",
        "                    pbar.set_postfix({'episode': '%d' % (num_episodes/10 * i + i_episode+1), 'return': '%.3f' % np.mean(return_list[-10:])})\n",
        "                pbar.update(1)\n",
        "    return return_list\n",
        "\n",
        "def train_off_policy_agent(env, agent, num_episodes, replay_buffer, minimal_size, batch_size):\n",
        "    return_list = []\n",
        "    for i in range(10):\n",
        "        with tqdm(total=int(num_episodes/10), desc='Iteration %d' % i) as pbar:\n",
        "            for i_episode in range(int(num_episodes/10)):\n",
        "                episode_return = 0\n",
        "                state = env.reset()\n",
        "                done = False\n",
        "                while not done:\n",
        "                    action = agent.take_action(state)\n",
        "                    next_state, reward, done, _ = env.step(action)\n",
        "                    replay_buffer.add(state, action, reward, next_state, done)\n",
        "                    state = next_state\n",
        "                    episode_return += reward\n",
        "                    if replay_buffer.size() > minimal_size:\n",
        "                        b_s, b_a, b_r, b_ns, b_d = replay_buffer.sample(batch_size)\n",
        "                        transition_dict = {'states': b_s, 'actions': b_a, 'next_states': b_ns, 'rewards': b_r, 'dones': b_d}\n",
        "                        agent.update(transition_dict)\n",
        "                return_list.append(episode_return)\n",
        "                if (i_episode+1) % 10 == 0:\n",
        "                    pbar.set_postfix({'episode': '%d' % (num_episodes/10 * i + i_episode+1), 'return': '%.3f' % np.mean(return_list[-10:])})\n",
        "                pbar.update(1)\n",
        "    return return_list\n",
        "\n",
        "\n",
        "def compute_advantage(gamma, lmbda, td_delta):\n",
        "    td_delta = td_delta.detach().numpy()\n",
        "    advantage_list = []\n",
        "    advantage = 0.0\n",
        "    for delta in td_delta[::-1]:\n",
        "        advantage = gamma * lmbda * advantage + delta\n",
        "        advantage_list.append(advantage)\n",
        "    advantage_list.reverse()\n",
        "    return torch.tensor(advantage_list, dtype=torch.float)\n"
      ],
      "metadata": {
        "id": "28WtOXXpF0iD"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNet(torch.nn.Module):\n",
        "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
        "        super(PolicyNet, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return F.softmax(self.fc2(x), dim=1)"
      ],
      "metadata": {
        "id": "wg6VuQoYF4fo"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ValueNet(torch.nn.Module):\n",
        "    def __init__(self, state_dim, hidden_dim):\n",
        "        super(ValueNet, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "jtOQHFjGGkvu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## TRPO"
      ],
      "metadata": {
        "id": "HtvCpz0eF80A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TRPO:\n",
        "    \"\"\" TRPO算法 \"\"\"\n",
        "    def __init__(self, hidden_dim, state_space, action_space, lmbda,\n",
        "                 kl_constraint, alpha, critic_lr, gamma, device):\n",
        "        state_dim = state_space.shape[0]\n",
        "        action_dim = action_space.n\n",
        "        # 策略网络参数不需要优化器更新\n",
        "        self.actor = PolicyNet(state_dim, hidden_dim, action_dim).to(device)\n",
        "        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n",
        "                                                 lr=critic_lr)\n",
        "        self.gamma = gamma\n",
        "        self.lmbda = lmbda  # GAE参数\n",
        "        self.kl_constraint = kl_constraint  # KL距离最大限制\n",
        "        self.alpha = alpha  # 线性搜索参数\n",
        "        self.device = device\n",
        "\n",
        "    def take_action(self, state):\n",
        "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
        "        probs = self.actor(state)\n",
        "        action_dist = torch.distributions.Categorical(probs)\n",
        "        action = action_dist.sample()\n",
        "        return action.item()\n",
        "\n",
        "    def hessian_matrix_vector_product(self, states, old_action_dists, vector):\n",
        "        # 计算黑塞矩阵和一个向量的乘积\n",
        "        new_action_dists = torch.distributions.Categorical(self.actor(states))\n",
        "        kl = torch.mean(\n",
        "            torch.distributions.kl.kl_divergence(old_action_dists,\n",
        "                                                 new_action_dists))  # 计算平均KL距离\n",
        "        kl_grad = torch.autograd.grad(kl,\n",
        "                                      self.actor.parameters(),\n",
        "                                      create_graph=True)\n",
        "        kl_grad_vector = torch.cat([grad.view(-1) for grad in kl_grad])\n",
        "        # KL距离的梯度先和向量进行点积运算\n",
        "        kl_grad_vector_product = torch.dot(kl_grad_vector, vector)\n",
        "        grad2 = torch.autograd.grad(kl_grad_vector_product,\n",
        "                                    self.actor.parameters())\n",
        "        grad2_vector = torch.cat([grad.view(-1) for grad in grad2])\n",
        "        return grad2_vector\n",
        "\n",
        "    def conjugate_gradient(self, grad, states, old_action_dists):  # 共轭梯度法求解方程\n",
        "        x = torch.zeros_like(grad)\n",
        "        r = grad.clone()\n",
        "        p = grad.clone()\n",
        "        rdotr = torch.dot(r, r)\n",
        "        for i in range(10):  # 共轭梯度主循环\n",
        "            Hp = self.hessian_matrix_vector_product(states, old_action_dists,\n",
        "                                                    p)\n",
        "            alpha = rdotr / torch.dot(p, Hp)\n",
        "            x += alpha * p\n",
        "            r -= alpha * Hp\n",
        "            new_rdotr = torch.dot(r, r)\n",
        "            if new_rdotr < 1e-10:\n",
        "                break\n",
        "            beta = new_rdotr / rdotr\n",
        "            p = r + beta * p\n",
        "            rdotr = new_rdotr\n",
        "        return x\n",
        "\n",
        "    def compute_surrogate_obj(self, states, actions, advantage, old_log_probs,\n",
        "                              actor):  # 计算策略目标\n",
        "        log_probs = torch.log(actor(states).gather(1, actions))\n",
        "        ratio = torch.exp(log_probs - old_log_probs)\n",
        "        return torch.mean(ratio * advantage)\n",
        "\n",
        "    def line_search(self, states, actions, advantage, old_log_probs,\n",
        "                    old_action_dists, max_vec):  # 线性搜索\n",
        "        old_para = torch.nn.utils.convert_parameters.parameters_to_vector(\n",
        "            self.actor.parameters())\n",
        "        old_obj = self.compute_surrogate_obj(states, actions, advantage,\n",
        "                                             old_log_probs, self.actor)\n",
        "        for i in range(15):  # 线性搜索主循环\n",
        "            coef = self.alpha**i\n",
        "            new_para = old_para + coef * max_vec\n",
        "            new_actor = copy.deepcopy(self.actor)\n",
        "            torch.nn.utils.convert_parameters.vector_to_parameters(\n",
        "                new_para, new_actor.parameters())\n",
        "            new_action_dists = torch.distributions.Categorical(\n",
        "                new_actor(states))\n",
        "            kl_div = torch.mean(\n",
        "                torch.distributions.kl.kl_divergence(old_action_dists,\n",
        "                                                     new_action_dists))\n",
        "            new_obj = self.compute_surrogate_obj(states, actions, advantage,\n",
        "                                                 old_log_probs, new_actor)\n",
        "            if new_obj > old_obj and kl_div < self.kl_constraint:\n",
        "                return new_para\n",
        "        return old_para\n",
        "\n",
        "    def policy_learn(self, states, actions, old_action_dists, old_log_probs,\n",
        "                     advantage):  # 更新策略函数\n",
        "        surrogate_obj = self.compute_surrogate_obj(states, actions, advantage,\n",
        "                                                   old_log_probs, self.actor)\n",
        "        grads = torch.autograd.grad(surrogate_obj, self.actor.parameters())\n",
        "        obj_grad = torch.cat([grad.view(-1) for grad in grads]).detach()\n",
        "        # 用共轭梯度法计算x = H^(-1)g\n",
        "        descent_direction = self.conjugate_gradient(obj_grad, states,\n",
        "                                                    old_action_dists)\n",
        "\n",
        "        Hd = self.hessian_matrix_vector_product(states, old_action_dists,\n",
        "                                                descent_direction)\n",
        "        max_coef = torch.sqrt(2 * self.kl_constraint /\n",
        "                              (torch.dot(descent_direction, Hd) + 1e-8))\n",
        "        new_para = self.line_search(states, actions, advantage, old_log_probs,\n",
        "                                    old_action_dists,\n",
        "                                    descent_direction * max_coef)  # 线性搜索\n",
        "        torch.nn.utils.convert_parameters.vector_to_parameters(\n",
        "            new_para, self.actor.parameters())  # 用线性搜索后的参数更新策略\n",
        "\n",
        "    def update(self, transition_dict):\n",
        "        states = torch.tensor(transition_dict['states'],\n",
        "                              dtype=torch.float).to(self.device)\n",
        "        actions = torch.tensor(transition_dict['actions']).view(-1, 1).to(\n",
        "            self.device)\n",
        "        rewards = torch.tensor(transition_dict['rewards'],\n",
        "                               dtype=torch.float).view(-1, 1).to(self.device)\n",
        "        next_states = torch.tensor(transition_dict['next_states'],\n",
        "                                   dtype=torch.float).to(self.device)\n",
        "        dones = torch.tensor(transition_dict['dones'],\n",
        "                             dtype=torch.float).view(-1, 1).to(self.device)\n",
        "        td_target = rewards + self.gamma * self.critic(next_states) * (1 -\n",
        "                                                                       dones)\n",
        "        td_delta = td_target - self.critic(states)\n",
        "        advantage = compute_advantage(self.gamma, self.lmbda,\n",
        "                                      td_delta.cpu()).to(self.device)\n",
        "        old_log_probs = torch.log(self.actor(states).gather(1,\n",
        "                                                            actions)).detach()\n",
        "        old_action_dists = torch.distributions.Categorical(\n",
        "            self.actor(states).detach())\n",
        "        critic_loss = torch.mean(\n",
        "            F.mse_loss(self.critic(states), td_target.detach()))\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()  # 更新价值函数\n",
        "        # 更新策略函数\n",
        "        self.policy_learn(states, actions, old_action_dists, old_log_probs,\n",
        "                          advantage)"
      ],
      "metadata": {
        "id": "X0ilcetnF6PC"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 1000\n",
        "hidden_dim = 128\n",
        "gamma = 0.98\n",
        "lmbda = 0.95\n",
        "critic_lr = 5e-3\n",
        "kl_constraint = 0.0005\n",
        "alpha = 0.5\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
        "    \"cpu\")\n",
        "\n",
        "env_name = 'CartPole-v1'\n",
        "env = gym.make(env_name)\n",
        "env.seed(0)\n",
        "torch.manual_seed(0)\n",
        "agent = TRPO(hidden_dim, env.observation_space, env.action_space, lmbda,\n",
        "             kl_constraint, alpha, critic_lr, gamma, device)\n",
        "return_list = train_on_policy_agent(env, agent, num_episodes)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ewWDgcIeGAgf",
        "outputId": "e9921f6e-9fb3-4553-ca4a-6bad5feaeb89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "Iteration 0:   0%|          | 0/100 [00:00<?, ?it/s]<ipython-input-6-29bd8b5dbd7f>:19: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:261.)\n",
            "  state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
            "Iteration 0: 100%|██████████| 100/100 [00:10<00:00,  9.24it/s, episode=100, return=40.900]\n",
            "Iteration 1: 100%|██████████| 100/100 [00:09<00:00, 10.97it/s, episode=200, return=55.300]\n",
            "Iteration 2: 100%|██████████| 100/100 [00:08<00:00, 11.22it/s, episode=300, return=56.300]\n",
            "Iteration 3: 100%|██████████| 100/100 [00:10<00:00,  9.58it/s, episode=400, return=76.200]\n",
            "Iteration 4: 100%|██████████| 100/100 [00:11<00:00,  8.49it/s, episode=500, return=93.900]\n",
            "Iteration 5: 100%|██████████| 100/100 [00:11<00:00,  8.37it/s, episode=600, return=84.600]\n",
            "Iteration 6: 100%|██████████| 100/100 [00:12<00:00,  8.14it/s, episode=700, return=108.300]\n",
            "Iteration 7: 100%|██████████| 100/100 [00:13<00:00,  7.62it/s, episode=800, return=121.700]\n",
            "Iteration 8:  38%|███▊      | 38/100 [00:05<00:08,  7.39it/s, episode=830, return=123.400]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episodes_list = list(range(len(return_list)))\n",
        "plt.plot(episodes_list, return_list)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Returns')\n",
        "plt.title('TRPO on {}'.format(env_name))\n",
        "plt.show()\n",
        "\n",
        "mv_return = moving_average(return_list, 9)\n",
        "plt.plot(episodes_list, mv_return)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Returns')\n",
        "plt.title('TRPO on {}'.format(env_name))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "EL0FtMiMGPhB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Continuous Env"
      ],
      "metadata": {
        "id": "gPSKw9nfJ-qd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNetContinuous(torch.nn.Module):\n",
        "    def __init__(self, state_dim, hidden_dim, action_dim):\n",
        "        super(PolicyNetContinuous, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(state_dim, hidden_dim)\n",
        "        self.fc_mu = torch.nn.Linear(hidden_dim, action_dim)\n",
        "        self.fc_std = torch.nn.Linear(hidden_dim, action_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        mu = 2.0 * torch.tanh(self.fc_mu(x))\n",
        "        std = F.softplus(self.fc_std(x))\n",
        "        return mu, std  # 高斯分布的均值和标准差\n",
        "\n",
        "\n",
        "class TRPOContinuous:\n",
        "    \"\"\" 处理连续动作的TRPO算法 \"\"\"\n",
        "    def __init__(self, hidden_dim, state_space, action_space, lmbda,\n",
        "                 kl_constraint, alpha, critic_lr, gamma, device):\n",
        "        state_dim = state_space.shape[0]\n",
        "        action_dim = action_space.shape[0]\n",
        "        self.actor = PolicyNetContinuous(state_dim, hidden_dim,\n",
        "                                         action_dim).to(device)\n",
        "        self.critic = ValueNet(state_dim, hidden_dim).to(device)\n",
        "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),\n",
        "                                                 lr=critic_lr)\n",
        "        self.gamma = gamma\n",
        "        self.lmbda = lmbda\n",
        "        self.kl_constraint = kl_constraint\n",
        "        self.alpha = alpha\n",
        "        self.device = device\n",
        "\n",
        "    def take_action(self, state):\n",
        "        state = torch.tensor([state], dtype=torch.float).to(self.device)\n",
        "        mu, std = self.actor(state)\n",
        "        action_dist = torch.distributions.Normal(mu, std)\n",
        "        action = action_dist.sample()\n",
        "        return [action.item()]\n",
        "\n",
        "    def hessian_matrix_vector_product(self,\n",
        "                                      states,\n",
        "                                      old_action_dists,\n",
        "                                      vector,\n",
        "                                      damping=0.1):\n",
        "        mu, std = self.actor(states)\n",
        "        new_action_dists = torch.distributions.Normal(mu, std)\n",
        "        kl = torch.mean(\n",
        "            torch.distributions.kl.kl_divergence(old_action_dists,\n",
        "                                                 new_action_dists))\n",
        "        kl_grad = torch.autograd.grad(kl,\n",
        "                                      self.actor.parameters(),\n",
        "                                      create_graph=True)\n",
        "        kl_grad_vector = torch.cat([grad.view(-1) for grad in kl_grad])\n",
        "        kl_grad_vector_product = torch.dot(kl_grad_vector, vector)\n",
        "        grad2 = torch.autograd.grad(kl_grad_vector_product,\n",
        "                                    self.actor.parameters())\n",
        "        grad2_vector = torch.cat(\n",
        "            [grad.contiguous().view(-1) for grad in grad2])\n",
        "        return grad2_vector + damping * vector\n",
        "\n",
        "    def conjugate_gradient(self, grad, states, old_action_dists):\n",
        "        x = torch.zeros_like(grad)\n",
        "        r = grad.clone()\n",
        "        p = grad.clone()\n",
        "        rdotr = torch.dot(r, r)\n",
        "        for i in range(10):\n",
        "            Hp = self.hessian_matrix_vector_product(states, old_action_dists,\n",
        "                                                    p)\n",
        "            alpha = rdotr / torch.dot(p, Hp)\n",
        "            x += alpha * p\n",
        "            r -= alpha * Hp\n",
        "            new_rdotr = torch.dot(r, r)\n",
        "            if new_rdotr < 1e-10:\n",
        "                break\n",
        "            beta = new_rdotr / rdotr\n",
        "            p = r + beta * p\n",
        "            rdotr = new_rdotr\n",
        "        return x\n",
        "\n",
        "    def compute_surrogate_obj(self, states, actions, advantage, old_log_probs,\n",
        "                              actor):\n",
        "        mu, std = actor(states)\n",
        "        action_dists = torch.distributions.Normal(mu, std)\n",
        "        log_probs = action_dists.log_prob(actions)\n",
        "        ratio = torch.exp(log_probs - old_log_probs)\n",
        "        return torch.mean(ratio * advantage)\n",
        "\n",
        "    def line_search(self, states, actions, advantage, old_log_probs,\n",
        "                    old_action_dists, max_vec):\n",
        "        old_para = torch.nn.utils.convert_parameters.parameters_to_vector(\n",
        "            self.actor.parameters())\n",
        "        old_obj = self.compute_surrogate_obj(states, actions, advantage,\n",
        "                                             old_log_probs, self.actor)\n",
        "        for i in range(15):\n",
        "            coef = self.alpha**i\n",
        "            new_para = old_para + coef * max_vec\n",
        "            new_actor = copy.deepcopy(self.actor)\n",
        "            torch.nn.utils.convert_parameters.vector_to_parameters(\n",
        "                new_para, new_actor.parameters())\n",
        "            mu, std = new_actor(states)\n",
        "            new_action_dists = torch.distributions.Normal(mu, std)\n",
        "            kl_div = torch.mean(\n",
        "                torch.distributions.kl.kl_divergence(old_action_dists,\n",
        "                                                     new_action_dists))\n",
        "            new_obj = self.compute_surrogate_obj(states, actions, advantage,\n",
        "                                                 old_log_probs, new_actor)\n",
        "            if new_obj > old_obj and kl_div < self.kl_constraint:\n",
        "                return new_para\n",
        "        return old_para\n",
        "\n",
        "    def policy_learn(self, states, actions, old_action_dists, old_log_probs,\n",
        "                     advantage):\n",
        "        surrogate_obj = self.compute_surrogate_obj(states, actions, advantage,\n",
        "                                                   old_log_probs, self.actor)\n",
        "        grads = torch.autograd.grad(surrogate_obj, self.actor.parameters())\n",
        "        obj_grad = torch.cat([grad.view(-1) for grad in grads]).detach()\n",
        "        descent_direction = self.conjugate_gradient(obj_grad, states,\n",
        "                                                    old_action_dists)\n",
        "        Hd = self.hessian_matrix_vector_product(states, old_action_dists,\n",
        "                                                descent_direction)\n",
        "        max_coef = torch.sqrt(2 * self.kl_constraint /\n",
        "                              (torch.dot(descent_direction, Hd) + 1e-8))\n",
        "        new_para = self.line_search(states, actions, advantage, old_log_probs,\n",
        "                                    old_action_dists,\n",
        "                                    descent_direction * max_coef)\n",
        "        torch.nn.utils.convert_parameters.vector_to_parameters(\n",
        "            new_para, self.actor.parameters())\n",
        "\n",
        "    def update(self, transition_dict):\n",
        "        states = torch.tensor(transition_dict['states'],\n",
        "                              dtype=torch.float).to(self.device)\n",
        "        actions = torch.tensor(transition_dict['actions'],\n",
        "                               dtype=torch.float).view(-1, 1).to(self.device)\n",
        "        rewards = torch.tensor(transition_dict['rewards'],\n",
        "                               dtype=torch.float).view(-1, 1).to(self.device)\n",
        "        next_states = torch.tensor(transition_dict['next_states'],\n",
        "                                   dtype=torch.float).to(self.device)\n",
        "        dones = torch.tensor(transition_dict['dones'],\n",
        "                             dtype=torch.float).view(-1, 1).to(self.device)\n",
        "        rewards = (rewards + 8.0) / 8.0  # 对奖励进行修改,方便训练\n",
        "        td_target = rewards + self.gamma * self.critic(next_states) * (1 -\n",
        "                                                                       dones)\n",
        "        td_delta = td_target - self.critic(states)\n",
        "        advantage = compute_advantage(self.gamma, self.lmbda,\n",
        "                                      td_delta.cpu()).to(self.device)\n",
        "        mu, std = self.actor(states)\n",
        "        old_action_dists = torch.distributions.Normal(mu.detach(),\n",
        "                                                      std.detach())\n",
        "        old_log_probs = old_action_dists.log_prob(actions)\n",
        "        critic_loss = torch.mean(\n",
        "            F.mse_loss(self.critic(states), td_target.detach()))\n",
        "        self.critic_optimizer.zero_grad()\n",
        "        critic_loss.backward()\n",
        "        self.critic_optimizer.step()\n",
        "        self.policy_learn(states, actions, old_action_dists, old_log_probs,\n",
        "                          advantage)"
      ],
      "metadata": {
        "id": "I2NBc12cHI65"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_episodes = 3000\n",
        "hidden_dim = 128\n",
        "gamma = 0.9\n",
        "lmbda = 0.9\n",
        "critic_lr = 1e-2\n",
        "kl_constraint = 0.00005\n",
        "alpha = 0.5\n",
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\n",
        "    \"cpu\")\n",
        "\n",
        "env_name = 'Pendulum-v1'\n",
        "env = gym.make(env_name)\n",
        "env.seed(0)\n",
        "torch.manual_seed(0)\n",
        "agent = TRPOContinuous(hidden_dim, env.observation_space, env.action_space,\n",
        "                       lmbda, kl_constraint, alpha, critic_lr, gamma, device)\n",
        "return_list = train_on_policy_agent(env, agent, num_episodes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74XW6_snKAw8",
        "outputId": "c58cf15a-e4d9-4bdb-8f19-f0ee970dd733"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:256: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed)` instead.\u001b[0m\n",
            "  deprecation(\n",
            "Iteration 0:  76%|███████▋  | 229/300 [00:39<00:11,  5.95it/s, episode=230, return=-1317.832]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episodes_list = list(range(len(return_list)))\n",
        "plt.plot(episodes_list, return_list)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Returns')\n",
        "plt.title('TRPO on {}'.format(env_name))\n",
        "plt.show()\n",
        "\n",
        "mv_return = moving_average(return_list, 9)\n",
        "plt.plot(episodes_list, mv_return)\n",
        "plt.xlabel('Episodes')\n",
        "plt.ylabel('Returns')\n",
        "plt.title('TRPO on {}'.format(env_name))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "YRNoq31CKHpH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NOTE\n",
        "\n",
        " # Google Colab Pro will not help the code.\n",
        " It is a waste of time (feel like crashed more often and have to restart from the beginning) and money\n",
        "\n",
        " When you view the code it will say run time disconnected but it still counts your usage"
      ],
      "metadata": {
        "id": "E5wfU46XWOdv"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hj3a2oMcKIts"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}